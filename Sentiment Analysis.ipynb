{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fe80ee",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "We will be analysing a sentiment analysis problem Airline tweets data. This is a multi class classification having an imbalanced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61e3793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "#For data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "871de085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.read_csv('AirlineTweets.csv')\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b318d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_[['airline_sentiment','text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31025010",
   "metadata": {},
   "source": [
    "### Preprocessing and cleaning of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aa566b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag_words(text):\n",
    "    result = re.sub('\\@\\w+','',text)\n",
    "    return result\n",
    "\n",
    "def remove_html_in_text(text):\n",
    "    result_wo_chars = re.sub(r'https?://\\S+|www\\.\\S+','', text)\n",
    "    result = re.sub('\\&\\w+','',result_wo_chars)\n",
    "    return result\n",
    "\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+','',text)\n",
    "    return result\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = word_tokenize(text)\n",
    "    words = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    for punc in punctuation:\n",
    "        text.replace(punc,'')\n",
    "    return text\n",
    "\n",
    "def extract_only_letters(text):\n",
    "    text = re.sub('[^a-zA-Z]+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47e2df80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varakhedi\\AppData\\Local\\Temp\\ipykernel_20696\\2042120263.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: remove_tag_words(x))\n",
      "C:\\Users\\varakhedi\\AppData\\Local\\Temp\\ipykernel_20696\\2042120263.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: remove_html_in_text(x))\n",
      "C:\\Users\\varakhedi\\AppData\\Local\\Temp\\ipykernel_20696\\2042120263.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: remove_numbers(x))\n",
      "C:\\Users\\varakhedi\\AppData\\Local\\Temp\\ipykernel_20696\\2042120263.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
      "C:\\Users\\varakhedi\\AppData\\Local\\Temp\\ipykernel_20696\\2042120263.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: extract_only_letters(x))\n",
      "C:\\Users\\varakhedi\\AppData\\Local\\Temp\\ipykernel_20696\\2042120263.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: lemmatize(x))\n",
      "C:\\Users\\varakhedi\\AppData\\Local\\Temp\\ipykernel_20696\\2042120263.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: remove_stop_words(x))\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda x: remove_tag_words(x))\n",
    "df['text'] = df['text'].apply(lambda x: remove_html_in_text(x))\n",
    "df['text'] = df['text'].apply(lambda x: remove_numbers(x))\n",
    "df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
    "df['text'] = df['text'].apply(lambda x: extract_only_letters(x))\n",
    "df['text'] = df['text'].apply(lambda x: lemmatize(x))\n",
    "df['text'] = df['text'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e1586",
   "metadata": {},
   "source": [
    "## Categorizing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afc12eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varakhedi\\AppData\\Local\\Temp\\ipykernel_20696\\3776861599.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['target'] = df['airline_sentiment'].map(target_map)\n"
     ]
    }
   ],
   "source": [
    "target_map={'negative':0, 'positive': 1, 'neutral': 2}\n",
    "df['target'] = df['airline_sentiment'].map(target_map)\n",
    "vectorizer = TfidfVectorizer(max_features = 2000)\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c81b6",
   "metadata": {},
   "source": [
    "## Checking data for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c56a6124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='count'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data = df, x = 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305e106",
   "metadata": {},
   "source": [
    "As we can see from the above histogram that there is a class imbalance towards negative tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af8a21",
   "metadata": {},
   "source": [
    "## Checking model performances with imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd26776",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "735b1ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report Logistic Regression without resampling- \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90      7308\n",
      "           1       0.87      0.71      0.78      1919\n",
      "           2       0.79      0.62      0.70      2485\n",
      "\n",
      "    accuracy                           0.85     11712\n",
      "   macro avg       0.84      0.76      0.79     11712\n",
      "weighted avg       0.84      0.85      0.84     11712\n",
      "\n",
      "\n",
      "\n",
      " F1 Score -  84.61\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 0)\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(\"Classification report Logistic Regression without resampling- \\n\\n\", classification_report(y_train, y_train_pred))\n",
    "f1_train_lr = round(f1_score(y_train, y_train_pred, average='micro')*100,2)\n",
    "print( \"\\n\\n F1 Score - \", f1_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3ceb182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report Logistic Regression without resampling- \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87      1870\n",
      "           1       0.79      0.59      0.68       444\n",
      "           2       0.66      0.53      0.59       614\n",
      "\n",
      "    accuracy                           0.79      2928\n",
      "   macro avg       0.76      0.68      0.71      2928\n",
      "weighted avg       0.78      0.79      0.78      2928\n",
      "\n",
      "\n",
      "\n",
      " F1 Score -  79.13\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "print(\"Classification report Logistic Regression without resampling- \\n\\n\", classification_report(y_test, y_test_pred))\n",
    "f1_lr = round(f1_score(y_test, y_test_pred, average='micro')*100,2)\n",
    "print( \"\\n\\n F1 Score - \", f1_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90472998",
   "metadata": {},
   "source": [
    "#### We can see here that the model performed well on training dataset but not testing dataset\n",
    "\n",
    "Lets try other models for their performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35fec6",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "711dbe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report Naive Bayes with Imbalanced data- \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.98      0.85      1870\n",
      "           1       0.87      0.43      0.57       444\n",
      "           2       0.74      0.31      0.44       614\n",
      "\n",
      "    accuracy                           0.75      2928\n",
      "   macro avg       0.79      0.57      0.62      2928\n",
      "weighted avg       0.76      0.75      0.72      2928\n",
      "\n",
      "\n",
      "\n",
      " F1 Score -  75.44\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "y_mnb_pred = mnb.predict(X_test)\n",
    "print(\"Classification report Naive Bayes with Imbalanced data- \\n\\n\", classification_report(y_test, y_mnb_pred))\n",
    "f1_mnb = round(f1_score(y_test, y_mnb_pred, average='micro')*100,2)\n",
    "print( \"\\n\\n F1 Score - \", f1_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88c044",
   "metadata": {},
   "source": [
    " ### Linear SVC model using OneVsRest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87435af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SVC Model with OVR with balanced data- \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87      1870\n",
      "           1       0.78      0.61      0.68       444\n",
      "           2       0.70      0.45      0.55       614\n",
      "\n",
      "    accuracy                           0.79      2928\n",
      "   macro avg       0.76      0.67      0.70      2928\n",
      "weighted avg       0.78      0.79      0.77      2928\n",
      "\n",
      "\n",
      "\n",
      " F1 Score -  78.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "svc = SVC()\n",
    "ovr_model = OneVsRestClassifier(svc)\n",
    "ovr_model.fit(X_train, y_train)\n",
    "y_ovr_pred = ovr_model.predict(X_test)\n",
    "print(\"Classification report SVC Model with OVR with balanced data- \\n\\n\", classification_report(y_test, y_ovr_pred))\n",
    "f1_ovr = round(f1_score(y_test, y_ovr_pred, average='micro')*100,2)\n",
    "print( \"\\n\\n F1 Score - \", f1_ovr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad75e1",
   "metadata": {},
   "source": [
    "### Linear SVC model using OneVsOne classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a8b3bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SVC Model with OVR with balanced data- \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87      1870\n",
      "           1       0.79      0.60      0.69       444\n",
      "           2       0.70      0.47      0.56       614\n",
      "\n",
      "    accuracy                           0.79      2928\n",
      "   macro avg       0.77      0.67      0.70      2928\n",
      "weighted avg       0.78      0.79      0.78      2928\n",
      "\n",
      "\n",
      "\n",
      " F1 Score -  78.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "ovo_model = OneVsOneClassifier(svc)\n",
    "ovo_model.fit(X_train, y_train)\n",
    "y_ovo_pred = ovo_model.predict(X_test)\n",
    "print(\"Classification report SVC Model with OVR with balanced data- \\n\\n\", classification_report(y_test, y_ovo_pred))\n",
    "f1_ovo = round(f1_score(y_test, y_ovo_pred, average='micro')*100,2)\n",
    "print( \"\\n\\n F1 Score - \", f1_ovo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "774068d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>79.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MNB</td>\n",
       "      <td>75.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC:OVR</td>\n",
       "      <td>78.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC:OVO</td>\n",
       "      <td>78.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Models  F1 Score\n",
       "0       LR     79.13\n",
       "1      MNB     75.44\n",
       "2  SVC:OVR     78.76\n",
       "3  SVC:OVO     78.89"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_f1_scores = {\"Models\": [\"LR\", \"MNB\", \"SVC:OVR\", \"SVC:OVO\"], \"F1 Score\" :[ f1_lr, f1_mnb,f1_ovr, f1_ovo] }\n",
    "df_f1_score = pd.DataFrame(model_f1_scores)\n",
    "df_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64042d0d",
   "metadata": {},
   "source": [
    "\n",
    "## Oversampling using SMOTE method\n",
    "\n",
    "We will use this SMOTE of Oversampling data to overcome the imbalance of target classes amd check performance of all above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a533ad35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD1CAYAAAC87SVQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAM9UlEQVR4nO3cX6jk5X3H8fenu9WaiFTxKGbXZC3dNlVLa12saaCUWnBLQtcbYQupSxAWxPwrhVZ7k6stFkpphCpdktS1DVkWG3BJ0VS29aJUNMcoNevWukSjp270pPSP9sJkzbcX5wkddmf3zGl256jf9wuG+c0zv9/MMwznPcNzZiZVhSSphx9b7wlIkubH6EtSI0Zfkhox+pLUiNGXpEaMviQ1snG9J7Caiy++uLZs2bLe05Ckd5Qnn3zyu1W1cOL42z76W7ZsYXFxcb2nIUnvKEm+PW3c5R1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY287b+cNW9b7vjb9Z7CWfPiXR9Z7ymcVe/m5w58/t7p3i7Pn+/0JakRoy9JjRh9SWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1MhM0U/yu0kOJ/lmki8n+YkkFyV5JMnz4/zCif3vTHI0yXNJbpwYvzbJM+O6u5PkbDwoSdJ0q0Y/ySbgU8C2qroa2ADsBO4ADlXVVuDQuEySK8f1VwHbgXuSbBg3dy+wG9g6TtvP6KORJJ3WrMs7G4HzkmwE3gO8AuwA9o3r9wE3je0dwP6qerOqXgCOAtcluQy4oKoeq6oC7p84RpI0B6tGv6r+DfgT4CXgGPBfVfV3wKVVdWzscwy4ZByyCXh54iaWxtimsX3iuCRpTmZZ3rmQlXfvVwDvA96b5GOnO2TKWJ1mfNp97k6ymGRxeXl5tSlKkmY0y/LObwAvVNVyVX0f+ArwK8CrY8mGcf7a2H8JuHzi+M2sLActje0Tx09SVXuraltVbVtYWFjL45EkncYs0X8JuD7Je8anbW4AjgAHgV1jn13Ag2P7ILAzyblJrmDlH7ZPjCWg15NcP27nloljJElzsHG1Harq8SQPAN8AjgNPAXuB84EDSW5l5YXh5rH/4SQHgGfH/rdX1Vvj5m4D7gPOAx4aJ0nSnKwafYCq+izw2ROG32TlXf+0/fcAe6aMLwJXr3GOkqQzxG/kSlIjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGZop+kp9M8kCSf0lyJMmHklyU5JEkz4/zCyf2vzPJ0STPJblxYvzaJM+M6+5OkrPxoCRJ0836Tv9zwMNV9UHgF4AjwB3AoaraChwal0lyJbATuArYDtyTZMO4nXuB3cDWcdp+hh6HJGkGq0Y/yQXArwJfAKiq71XVfwI7gH1jt33ATWN7B7C/qt6sqheAo8B1SS4DLqiqx6qqgPsnjpEkzcEs7/R/ClgG/jLJU0k+n+S9wKVVdQxgnF8y9t8EvDxx/NIY2zS2TxyXJM3JLNHfCPwScG9VXQP8D2Mp5xSmrdPXacZPvoFkd5LFJIvLy8szTFGSNItZor8ELFXV4+PyA6y8CLw6lmwY569N7H/5xPGbgVfG+OYp4yepqr1Vta2qti0sLMz6WCRJq1g1+lX1HeDlJD87hm4AngUOArvG2C7gwbF9ENiZ5NwkV7DyD9snxhLQ60muH5/auWXiGEnSHGyccb9PAl9Kcg7wLeDjrLxgHEhyK/AScDNAVR1OcoCVF4bjwO1V9da4nduA+4DzgIfGSZI0JzNFv6qeBrZNueqGU+y/B9gzZXwRuHoN85MknUF+I1eSGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNTJz9JNsSPJUkq+OyxcleSTJ8+P8wol970xyNMlzSW6cGL82yTPjuruT5Mw+HEnS6azlnf6ngSMTl+8ADlXVVuDQuEySK4GdwFXAduCeJBvGMfcCu4Gt47T9R5q9JGlNZop+ks3AR4DPTwzvAPaN7X3ATRPj+6vqzap6ATgKXJfkMuCCqnqsqgq4f+IYSdIczPpO/8+A3wd+MDF2aVUdAxjnl4zxTcDLE/stjbFNY/vEcUnSnKwa/SQfBV6rqidnvM1p6/R1mvFp97k7yWKSxeXl5RnvVpK0mlne6X8Y+K0kLwL7gV9P8tfAq2PJhnH+2th/Cbh84vjNwCtjfPOU8ZNU1d6q2lZV2xYWFtbwcCRJp7Nq9KvqzqraXFVbWPkH7d9X1ceAg8Cusdsu4MGxfRDYmeTcJFew8g/bJ8YS0OtJrh+f2rll4hhJ0hxs/BGOvQs4kORW4CXgZoCqOpzkAPAscBy4vareGsfcBtwHnAc8NE6SpDlZU/Sr6lHg0bH978ANp9hvD7BnyvgicPVaJylJOjP8Rq4kNWL0JakRoy9JjRh9SWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1Ijq0Y/yeVJ/iHJkSSHk3x6jF+U5JEkz4/zCyeOuTPJ0STPJblxYvzaJM+M6+5OkrPzsCRJ08zyTv848HtV9XPA9cDtSa4E7gAOVdVW4NC4zLhuJ3AVsB24J8mGcVv3AruBreO0/Qw+FknSKlaNflUdq6pvjO3XgSPAJmAHsG/stg+4aWzvAPZX1ZtV9QJwFLguyWXABVX1WFUVcP/EMZKkOVjTmn6SLcA1wOPApVV1DFZeGIBLxm6bgJcnDlsaY5vG9onj0+5nd5LFJIvLy8trmaIk6TRmjn6S84G/AT5TVf99ul2njNVpxk8erNpbVduqatvCwsKsU5QkrWKm6Cf5cVaC/6Wq+soYfnUs2TDOXxvjS8DlE4dvBl4Z45unjEuS5mSWT+8E+AJwpKr+dOKqg8Cusb0LeHBifGeSc5Ncwco/bJ8YS0CvJ7l+3OYtE8dIkuZg4wz7fBj4HeCZJE+PsT8E7gIOJLkVeAm4GaCqDic5ADzLyid/bq+qt8ZxtwH3AecBD42TJGlOVo1+Vf0j09fjAW44xTF7gD1TxheBq9cyQUnSmeM3ciWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI3OPfpLtSZ5LcjTJHfO+f0nqbK7RT7IB+HPgN4Ergd9OcuU85yBJnc37nf51wNGq+lZVfQ/YD+yY8xwkqa2Nc76/TcDLE5eXgF8+cacku4Hd4+IbSZ6bw9zWy8XAd+dxR/njedxLK3N77sDn7yx4tz9/H5g2OO/oZ8pYnTRQtRfYe/ans/6SLFbVtvWeh9bO5+6drevzN+/lnSXg8onLm4FX5jwHSWpr3tH/OrA1yRVJzgF2AgfnPAdJamuuyztVdTzJJ4CvARuAL1bV4XnO4W2oxTLWu5TP3Ttby+cvVSctqUuS3qX8Rq4kNWL0JakRoy9Jjcz7c/qtJfkgK19Qe7yq3pgY315VD6/fzKR3t/G3t4OVv79i5aPiB6vqyLpObB34Tn9OknwKeBD4JPDNJJM/P/FH6zMrnQlJPr7ec9CpJfkDVn7yJcATrHx0PMCXO/7oo5/emZMkzwAfqqo3kmwBHgD+qqo+l+SpqrpmfWeo/68kL1XV+9d7Hpouyb8CV1XV908YPwc4XFVb12dm68PlnfnZ8MMlnap6McmvAQ8k+QDTf55CbyNJ/vlUVwGXznMuWrMfAO8Dvn3C+GXjulaM/vx8J8kvVtXTAOMd/0eBLwI/v64z0ywuBW4E/uOE8QD/NP/paA0+AxxK8jz/94OP7wd+GvjEek1qvRj9+bkFOD45UFXHgVuS/MX6TElr8FXg/B++aE9K8ujcZ6OZVdXDSX6GlZ9238TKC/US8PWqemtdJ7cOXNOXpEb89I4kNWL0JakRoy9JjRh9SWrE6EtSI/8LiUsRrZcCIbQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "sm = SMOTE(random_state = 0)\n",
    "X_resample,y_resample = sm.fit_resample(X,y)\n",
    "y_resample.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bcfd2a",
   "metadata": {},
   "source": [
    "### Logistic Regression classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "891ab3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report Logistic Regression- \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84      1843\n",
      "           1       0.84      0.77      0.80      1840\n",
      "           2       0.72      0.83      0.77      1824\n",
      "\n",
      "    accuracy                           0.80      5507\n",
      "   macro avg       0.81      0.80      0.80      5507\n",
      "weighted avg       0.81      0.80      0.80      5507\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80.37"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resample_train, X_resample_test, y_resample_train, y_resample_test = train_test_split(X_resample, y_resample, test_size=0.2, random_state = 1)\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_resample_train, y_resample_train)\n",
    "y_resample_pred = model.predict(X_resample_test)\n",
    "print(\"Classification report Logistic Regression- \\n\\n\", classification_report(y_resample_test, y_resample_pred))\n",
    "f1_lr_sm = round(f1_score(y_resample_test, y_resample_pred, average = 'micro')*100,2)\n",
    "f1_lr_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66f1f2",
   "metadata": {},
   "source": [
    "### Naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1be98bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report Multinomial Naive Bayes- \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.78      1843\n",
      "           1       0.81      0.82      0.81      1840\n",
      "           2       0.74      0.69      0.71      1824\n",
      "\n",
      "    accuracy                           0.77      5507\n",
      "   macro avg       0.77      0.77      0.77      5507\n",
      "weighted avg       0.77      0.77      0.77      5507\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.92"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_resample_train, y_resample_train)\n",
    "y_mnb_pred = mnb.predict(X_resample_test)\n",
    "print(\"Classification report Multinomial Naive Bayes- \\n\\n\", classification_report(y_resample_test, y_mnb_pred))\n",
    "f1_nb_sm = round(f1_score(y_resample_test, y_mnb_pred, average = 'micro')*100,2)\n",
    "f1_nb_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152670a",
   "metadata": {},
   "source": [
    "### Linear SVC model using OneVsRest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d4e70a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SVC Model with OVR with balanced data- \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90      1843\n",
      "           1       0.92      0.84      0.87      1840\n",
      "           2       0.82      0.86      0.84      1824\n",
      "\n",
      "    accuracy                           0.87      5507\n",
      "   macro avg       0.87      0.87      0.87      5507\n",
      "weighted avg       0.87      0.87      0.87      5507\n",
      "\n",
      "\n",
      "\n",
      " F1 Score -  87.25\n"
     ]
    }
   ],
   "source": [
    "ovr_model = OneVsRestClassifier(svc)\n",
    "ovr_model.fit(X_resample_train, y_resample_train)\n",
    "y_ovr_sm_pred = ovr_model.predict(X_resample_test)\n",
    "print(\"Classification report SVC Model with OVR with balanced data- \\n\\n\", classification_report(y_resample_test, y_ovr_sm_pred))\n",
    "f1_sm_ovr = round(f1_score(y_resample_test, y_ovr_sm_pred, average='micro')*100,2)\n",
    "print( \"\\n\\n F1 Score - \", f1_sm_ovr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5a5b5",
   "metadata": {},
   "source": [
    "### Linear SVC model using OneVsOne classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a2cf562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SVC Model with OVR with balanced data- \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.89      1843\n",
      "           1       0.92      0.82      0.87      1840\n",
      "           2       0.83      0.83      0.83      1824\n",
      "\n",
      "    accuracy                           0.86      5507\n",
      "   macro avg       0.87      0.86      0.86      5507\n",
      "weighted avg       0.87      0.86      0.86      5507\n",
      "\n",
      "\n",
      "\n",
      " F1 Score -  86.27\n"
     ]
    }
   ],
   "source": [
    "ovo_model.fit(X_resample_train, y_resample_train)\n",
    "y_ovo_sm_pred = ovo_model.predict(X_resample_test)\n",
    "print(\"Classification report SVC Model with OVR with balanced data- \\n\\n\", classification_report(y_resample_test, y_ovo_sm_pred))\n",
    "f1_ovo_sm = round(f1_score(y_resample_test, y_ovo_sm_pred, average='micro')*100,2)\n",
    "print( \"\\n\\n F1 Score - \", f1_ovo_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00e37654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Scores after SMOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>79.13</td>\n",
       "      <td>80.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MNB</td>\n",
       "      <td>75.44</td>\n",
       "      <td>76.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC:OVR</td>\n",
       "      <td>78.76</td>\n",
       "      <td>87.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC:OVO</td>\n",
       "      <td>78.89</td>\n",
       "      <td>86.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Models  F1 Score  F1 Scores after SMOTE\n",
       "0       LR     79.13                  80.37\n",
       "1      MNB     75.44                  76.92\n",
       "2  SVC:OVR     78.76                  87.25\n",
       "3  SVC:OVO     78.89                  86.27"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_f1_scores = {\"Models\": [\"LR\", \"MNB\", \"SVC:OVR\", \"SVC:OVO\"], \"F1 Score\" :[ f1_lr, f1_mnb,f1_ovr, f1_ovo], \"F1 Scores after SMOTE\" : [f1_lr_sm, f1_nb_sm,f1_sm_ovr,f1_ovo_sm] }\n",
    "df_f1_score = pd.DataFrame(model_f1_scores)\n",
    "df_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6691c2",
   "metadata": {},
   "source": [
    "Looking at results we can see that SVC model outperforms other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3630fc6",
   "metadata": {},
   "source": [
    "### Now let us try the text Augmention method using nlpaug and review the results on how it affects the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a29ec2e8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\supriya\\lib\\site-packages (4.19.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\supriya\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\supriya\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\supriya\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\supriya\\lib\\site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\supriya\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\supriya\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in c:\\supriya\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\supriya\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\supriya\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\supriya\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\supriya\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\supriya\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\supriya\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\supriya\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\supriya\\lib\\site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\supriya\\lib\\site-packages (from requests->transformers) (2021.10.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Supriya\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.10-py3-none-any.whl (410 kB)\n",
      "     -------------------------------------- 410.8/410.8 KB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\supriya\\lib\\site-packages (from nlpaug) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\supriya\\lib\\site-packages (from nlpaug) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\supriya\\lib\\site-packages (from nlpaug) (1.20.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\supriya\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\supriya\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\supriya\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\supriya\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\supriya\\lib\\site-packages (from requests>=2.22.0->nlpaug) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\supriya\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\supriya\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->nlpaug) (1.16.0)\n",
      "Installing collected packages: nlpaug\n",
      "Successfully installed nlpaug-1.1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Supriya\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9106a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5b25d254ab47cfbd580172bd4be2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db7f533588a4ac8972fce35ee0c20fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1024505eb5d4ae5ab0540039af9d90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37eea805a7554f36b4c6117123d9dbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336eb669010c4806b38e3730ae15c22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nlpaug.augmenter.word.context_word_embs as aug\n",
    "augmenter = aug.ContextualWordEmbsAug(model_path = 'bert-base-uncased', action='insert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fcc1334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11712, 3)\n",
      "(2928, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size = 0.2, random_state = 0)\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff02ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df = df_train[df_train['target'] == 1]\n",
    "neutral_df = df_train[df_train['target'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7b74d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>yes... thankfully catering guys got loading fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>yes thankfully my catering crew got loading fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>mdw bay san flt first attendant melissa wa at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mdw san jose flt shuttle attendant melissa wa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>first in class the way headed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3829</th>\n",
       "      <td>1</td>\n",
       "      <td>but thank guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3830</th>\n",
       "      <td>1</td>\n",
       "      <td>beautiful view over flying san juan jose ca du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>1</td>\n",
       "      <td>beautiful view all flying san jose air ca by e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>1</td>\n",
       "      <td>hey thanks help with wish phone phone rep coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>1</td>\n",
       "      <td>hey thanks your help wish phone that rep could...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3834 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text\n",
       "0          1  yes... thankfully catering guys got loading fr...\n",
       "1          1  yes thankfully my catering crew got loading fr...\n",
       "2          1  mdw bay san flt first attendant melissa wa at ...\n",
       "3          1  mdw san jose flt shuttle attendant melissa wa ...\n",
       "4          1                      first in class the way headed\n",
       "...      ...                                                ...\n",
       "3829       1                                      but thank guy\n",
       "3830       1  beautiful view over flying san juan jose ca du...\n",
       "3831       1  beautiful view all flying san jose air ca by e...\n",
       "3832       1  hey thanks help with wish phone phone rep coul...\n",
       "3833       1  hey thanks your help wish phone that rep could...\n",
       "\n",
       "[3834 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_texts = []\n",
    "for _, row in positive_df.iterrows():\n",
    "    sentences = augmenter.augment(row['text'], n=2)\n",
    "    for text in sentences:\n",
    "        augmented_texts.append(text)\n",
    "data_positive = {\n",
    "    'target' : 1,\n",
    "    'text' : augmented_texts\n",
    "}\n",
    "\n",
    "aug_df_positive = pd.DataFrame(data_positive)\n",
    "aug_df_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6156dd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>think that dfw omg oh yeah and cold everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>that dfw mean omg of yeah... cold everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>however many minutes left i min wway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>too many left soon i min wway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>i that i did very little concerned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>2</td>\n",
       "      <td>but okay thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>2</td>\n",
       "      <td>sure chance u offer for fresh bottled guacamol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4959</th>\n",
       "      <td>2</td>\n",
       "      <td>double chance on u offer fresh cut guacamole f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4960</th>\n",
       "      <td>2</td>\n",
       "      <td>2 followback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4961</th>\n",
       "      <td>2</td>\n",
       "      <td>no followback</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4962 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text\n",
       "0          2     think that dfw omg oh yeah and cold everywhere\n",
       "1          2       that dfw mean omg of yeah... cold everywhere\n",
       "2          2               however many minutes left i min wway\n",
       "3          2                      too many left soon i min wway\n",
       "4          2                 i that i did very little concerned\n",
       "...      ...                                                ...\n",
       "4957       2                                     but okay thank\n",
       "4958       2  sure chance u offer for fresh bottled guacamol...\n",
       "4959       2  double chance on u offer fresh cut guacamole f...\n",
       "4960       2                                       2 followback\n",
       "4961       2                                      no followback\n",
       "\n",
       "[4962 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_neutral_texts = []\n",
    "for _, row in neutral_df.iterrows():\n",
    "    sentences = augmenter.augment(row['text'], n=2)\n",
    "    for text in sentences:\n",
    "        augmented_neutral_texts.append(text)\n",
    "data_neutral = {\n",
    "    'target' : 2,\n",
    "    'text' : augmented_neutral_texts\n",
    "}\n",
    "\n",
    "aug_df_neutral = pd.DataFrame(data_neutral)\n",
    "aug_df_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de9308f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['target','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d44779ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varakhedi\\AppData\\Local\\Temp\\ipykernel_20696\\968878850.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_aug = df_train.append(aug_df_positive)\n",
      "C:\\Users\\varakhedi\\AppData\\Local\\Temp\\ipykernel_20696\\968878850.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_aug = df_aug.append(aug_df_neutral)\n"
     ]
    }
   ],
   "source": [
    "df_aug = df_train.append(aug_df_positive)\n",
    "df_aug = df_aug.append(aug_df_neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f9918",
   "metadata": {},
   "source": [
    "#### Let's check the target class data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "95630afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    7447\n",
       "0    7308\n",
       "1    5753\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aug['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d1fdd",
   "metadata": {},
   "source": [
    "The positive and neutral class values have increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ace2f6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20508x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 166936 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 2000)\n",
    "X_aug_train = vectorizer.fit_transform(df_aug['text'])\n",
    "X_aug_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7603310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2928x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 23378 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_aug_test = vectorizer.fit_transform(df_test['text'])\n",
    "X_aug_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "13b87a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_aug_train = df_aug['target']\n",
    "y_aug_test = df_test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0706cc2b",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5f9548f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report Logistic Regression- \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.37      0.47      1870\n",
      "           1       0.15      0.14      0.14       444\n",
      "           2       0.24      0.53      0.33       614\n",
      "\n",
      "    accuracy                           0.37      2928\n",
      "   macro avg       0.34      0.35      0.31      2928\n",
      "weighted avg       0.47      0.37      0.39      2928\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37.16"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=500)\n",
    "lr.fit(X_aug_train, y_aug_train)\n",
    "y_aug_pred = lr.predict(X_aug_test)\n",
    "print(\"Classification report Logistic Regression- \\n\\n\", classification_report(y_aug_test, y_aug_pred))\n",
    "f1_lr_aug = round(f1_score(y_aug_test, y_aug_pred, average = 'micro')*100,2)\n",
    "f1_lr_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8f885",
   "metadata": {},
   "source": [
    "### Linear SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f4e92f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SVC Model with OVR with balanced data- \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.45      0.52      1870\n",
      "           1       0.15      0.14      0.14       444\n",
      "           2       0.24      0.48      0.32       614\n",
      "\n",
      "    accuracy                           0.41      2928\n",
      "   macro avg       0.34      0.35      0.33      2928\n",
      "weighted avg       0.48      0.41      0.42      2928\n",
      "\n",
      "\n",
      "\n",
      " F1 Score -  40.68\n"
     ]
    }
   ],
   "source": [
    "ovr_aug = OneVsRestClassifier(svc)\n",
    "ovr_aug.fit(X_aug_train, y_aug_train)\n",
    "y_ovr_aug_pred = ovr_aug.predict(X_aug_test)\n",
    "print(\"Classification report SVC Model with OVR with balanced data- \\n\\n\", classification_report(y_aug_test, y_ovr_aug_pred))\n",
    "f1_aug_ovr = round(f1_score(y_aug_test, y_ovr_aug_pred, average='micro')*100,2)\n",
    "print( \"\\n\\n F1 Score - \", f1_aug_ovr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6af5e8",
   "metadata": {},
   "source": [
    "### Looking at the results the Data Augmentation performed worst than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed3cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
